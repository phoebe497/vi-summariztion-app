import streamlit as st
import torch
from transformers import T5ForConditionalGeneration, T5Tokenizer
import time
import os

# Ki·ªÉm tra thi·∫øt b·ªã
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
if device.type == "cuda":
    print(f"ƒêang s·ª≠ d·ª•ng GPU: {torch.cuda.get_device_name(0)}")
else:
    print("ƒêang s·ª≠ d·ª•ng CPU")

# Kh·ªüi t·∫°o session state ƒë·ªÉ l∆∞u m√¥ h√¨nh v√† tokenizer
if "model" not in st.session_state:
    st.session_state.model = None
if "tokenizer" not in st.session_state:
    st.session_state.tokenizer = None

# H√†m load m√¥ h√¨nh (ch·ªâ g·ªçi khi c·∫ßn)
def load_model_and_tokenizer():
    model_name = "NlpHUST/t5-small-vi-summarization"
    try:
        if st.session_state.tokenizer is None:
            st.session_state.tokenizer = T5Tokenizer.from_pretrained(model_name)
        if st.session_state.model is None:
            model = T5ForConditionalGeneration.from_pretrained(
                model_name,
                torch_dtype=torch.float32
            )
            model = model.to(device)
            # Load m√¥ h√¨nh fine-tuned
            best_model_path = "D:/NLP/Research_T5/checkpoints/best_model.pt"
            if os.path.exists(best_model_path):
                if device.type == "cuda":
                    model.load_state_dict(torch.load(best_model_path))
                else:
                    model.load_state_dict(torch.load(best_model_path, map_location=torch.device('cpu')))
            else:
                st.error(f"Kh√¥ng t√¨m th·∫•y file checkpoint t·∫°i: {best_model_path}!")
                st.stop()
            model.eval()
            st.session_state.model = model
    except Exception as e:
        st.error(f"L·ªói khi load m√¥ h√¨nh: {e}")
        st.stop()

# H√†m t√≥m t·∫Øt
def summarize(text, model, tokenizer, device, max_length=150):
    model.eval()
    prefix = "T√≥m t·∫Øt: "
    inputs = tokenizer(prefix + text, return_tensors="pt", max_length=512, truncation=True)
    
    prefix_tokens = len(tokenizer(prefix, return_tensors="pt")["input_ids"][0])
    adjusted_max_length = max_length + prefix_tokens
    
    input_ids = inputs["input_ids"].to(device)
    attention_mask = inputs["attention_mask"].to(device)
    
    with torch.no_grad():
        summary_ids = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=adjusted_max_length,
            min_length=int(max_length * 0.6),  # Gi·∫£m min_length
            num_beams=2,  # Gi·∫£m num_beams ƒë·ªÉ tƒÉng t·ªëc
            early_stopping=True,  # B·∫≠t early stopping
            pad_token_id=tokenizer.pad_token_id
        )
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    
    # Post-processing: C·∫Øt ·ªü cu·ªëi c√¢u
    if summary and summary[-1] not in ['.', '!', '?']:
        last_sentence_end = max(summary.rfind('.'), summary.rfind('!'), summary.rfind('?'))
        if last_sentence_end != -1:
            summary = summary[:last_sentence_end + 1]
    
    return summary

# Giao di·ªán Streamlit
st.markdown(
    """
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Quicksand:wght@700&display=swap');
        .title-card {
            background-color: #FAFAFA;
            padding: 10px;
            border-radius: 15px;
            text-align: center;
            margin-bottom: 20px;
        }
        .title-text {
            margin-top: 0;
        }
        .title-text h1 {
            color: #FF4B4B;
            font-family: 'Quicksand', sans-serif;
            font-size: 36px;
            margin: 0;
            text-shadow: 2px 2px 5px #aaa;
        }
        .title-text p {
            font-size: 16px;
            color: #666;
            margin: 5px 0 0 0;
        }
        .summary-box {
            background-color: #F9F9F9;
            padding: 15px;
            border-radius: 10px;
            border-left: 5px solid #FF4B4B;
            font-size: 16px;
            font-weight: bold;
        }
    </style>
    <div class="title-card">
        <div class="title-text">
            <h1>AI T√≥m t·∫Øt vƒÉn b·∫£n Ti·∫øng Vi·ªát</h1>
            <p>·ª®ng d·ª•ng t√≥m t·∫Øt vƒÉn b·∫£n t·ª± ƒë·ªông s·ª≠ d·ª•ng m√¥ h√¨nh T5-small</p>
        </div>
    </div>
    """,
    unsafe_allow_html=True
)

st.write("### üìå Nh·∫≠p vƒÉn b·∫£n c·∫ßn t√≥m t·∫Øt:")

# √î nh·∫≠p vƒÉn b·∫£n
input_text = st.text_area("VƒÉn b·∫£n ƒë·∫ßu v√†o", height=200)

# Sidebar
with st.sidebar:
    st.markdown("### ‚öôÔ∏è T√πy ch·ªçn t√≥m t·∫Øt")
    length_option = st.selectbox("ƒê·ªô d√†i t√≥m t·∫Øt", ["Ng·∫Øn", "D√†i"])
    max_length_map = {
        "Ng·∫Øn": 80,  # Gi·∫£m ƒë·ªÉ tƒÉng t·ªëc
        "D√†i": 150   # Gi·∫£m ƒë·ªÉ tƒÉng t·ªëc
    }
    max_length = max_length_map[length_option]
    
    st.markdown("---")
    st.markdown("### üìå Th√¥ng tin m√¥ h√¨nh")
    st.write("- **M√¥ h√¨nh:** T5-small (NlpHUST/t5-small-vi-summarization)")
    st.write("- **Fine-tuned tr√™n:** 190 b√†i b√°o khoa h·ªçc ti·∫øng Vi·ªát")
    st.write("- **Batch size:** 128, **Epochs:** 200")
    st.write("- **Max input length:** 512")

# N√∫t t√≥m t·∫Øt
if st.button("üöÄ T√≥m t·∫Øt ngay"):
    if input_text.strip() == "":
        st.error("Vui l√≤ng nh·∫≠p vƒÉn b·∫£n ƒë·ªÉ t√≥m t·∫Øt!")
    else:
        # Load m√¥ h√¨nh n·∫øu ch∆∞a load
        if st.session_state.model is None or st.session_state.tokenizer is None:
            with st.spinner("‚è≥ ƒêang load m√¥ h√¨nh..."):
                load_model_and_tokenizer()
        
        # T√≥m t·∫Øt
        with st.spinner("‚è≥ ƒêang x·ª≠ l√Ω..."):
            progress_bar = st.progress(0)
            for i in range(100):
                time.sleep(0.01)
                progress_bar.progress(i + 1)
            summary = summarize(
                input_text,
                st.session_state.model,
                st.session_state.tokenizer,
                device,
                max_length=max_length
            )

        st.success("‚úÖ T√≥m t·∫Øt th√†nh c√¥ng!")
        st.write("### ‚ú® K·∫øt qu·∫£ t√≥m t·∫Øt:")
        st.markdown(f"<div class='summary-box'>{summary}</div>", unsafe_allow_html=True)

        # Th√¥ng tin ƒë·ªô d√†i
        st.write("### üìè Th√¥ng tin ƒë·ªô d√†i:")
        st.write(f"- ƒê·ªô d√†i vƒÉn b·∫£n g·ªëc: **{len(input_text.split())} t·ª´**, **{len(input_text)} k√Ω t·ª±**")
        st.write(f"- ƒê·ªô d√†i vƒÉn b·∫£n t√≥m t·∫Øt: **{len(summary.split())} t·ª´**, **{len(summary)} k√Ω t·ª±**")
        # summary_tokens = len(st.session_state.tokenizer(summary, return_tensors="pt")["input_ids"][0])
        # st.write(f"- S·ªë token t√≥m t·∫Øt: **{summary_tokens} tokens**")