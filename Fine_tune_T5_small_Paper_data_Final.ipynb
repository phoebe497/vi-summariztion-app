{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "OFdYH58ueEyn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in c:\\users\\84978\\anaconda3\\lib\\site-packages (4.51.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\84978\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\84978\\anaconda3\\lib\\site-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\84978\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\84978\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\84978\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\84978\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: requests in c:\\users\\84978\\anaconda3\\lib\\site-packages (from transformers) (2.32.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\84978\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\84978\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\users\\84978\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\84978\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\84978\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.11.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\84978\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\84978\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\84978\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\84978\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\84978\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.6.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DqWL9L50edOO"
      },
      "outputs": [],
      "source": [
        "# %pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "SRml2pDXef_y"
      },
      "outputs": [],
      "source": [
        "# %pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true,
        "id": "-f3Oo4sIei0Q"
      },
      "outputs": [],
      "source": [
        "# %pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "p2WjMgVgcj4K"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Không có GPU, sử dụng CPU.\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m   device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNlpHUST/t5-small-vi-summarization\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 19\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m T5Tokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNlpHUST/t5-small-vi-summarization\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Load pre-trained model (giữ nguyên để so sánh)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m pretrained_model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n",
            "File \u001b[1;32mc:\\Users\\84978\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1840\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1839\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1840\u001b[0m requires_backends(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_backends)\n",
            "File \u001b[1;32mc:\\Users\\84978\\anaconda3\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1828\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1826\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 1828\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
            "\u001b[1;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
          ]
        }
      ],
      "source": [
        "#Thực hiện import thư viện và model\n",
        "import os\n",
        "import torch\n",
        "from transformers import T5ForConditionalGeneration, T5Tokenizer, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "\n",
        "# Kiểm tra GPU\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print('Có %d GPU khả dụng.' % torch.cuda.device_count())\n",
        "  print('Sẽ sử dụng GPU:', torch.cuda.get_device_name(0))\n",
        "  torch.cuda.empty_cache()\n",
        "  print(\"Đã xóa bộ nhớ GPU trước khi tải mô hình\")\n",
        "else:\n",
        "  print('Không có GPU, sử dụng CPU.')\n",
        "  device = torch.device(\"cpu\")\n",
        "\n",
        "model_name = \"NlpHUST/t5-small-vi-summarization\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"NlpHUST/t5-small-vi-summarization\")\n",
        "\n",
        "# Load pre-trained model (giữ nguyên để so sánh)\n",
        "pretrained_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "pretrained_model.to(device)\n",
        "\n",
        "# Load checkpoint nếu tồn tại\n",
        "checkpoint_dir = \"Research_T5\\\\checkpoints\"\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.startswith(\"checkpoint_epoch_\")]\n",
        "start_epoch = 0\n",
        "if checkpoint_files:\n",
        "  latest_checkpoint = max(checkpoint_files, key=lambda x: int(x.split(\"_\")[-1].split(\".\")[0]))\n",
        "  checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
        "  checkpoint = torch.load(checkpoint_path)\n",
        "  model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  start_epoch = checkpoint['epoch'] + 1\n",
        "  print(f\"Loaded checkpoint from {checkpoint_path}, resuming from epoch {start_epoch+1}\")\n",
        "else:\n",
        "  model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "  model.to(device)\n",
        "\n",
        "\n",
        "src = \"Theo BHXH Việt Nam, nhiều doanh nghiệp vẫn chỉ đóng BHXH cho người lao động theo mức lương. Dù quy định từ 1/1/2018, tiền lương tháng đóng BHXH gồm mức lương và thêm khoản bổ sung khác.BHXH Việt Nam vừa có báo cáo về tình hình thực hiện chính sách BHXH thời gian qua.Theo đó, tình trạng nợ, trốn đóng BHXH, BHTN vẫn xảy ra ở hầu hết các tỉnh, thành. Thống kê tới ngày 31/12/2020, tổng số nợ BHXH, BHYT, BHTN là hơn 13.500 tỷ đồng, chiếm 3,35 % số phải thu, trong đó: Số nợ BHXH bắt buộc là hơn 8.600 tỷ đồng, nợ BHTN là 335 tỷ đồng. Liên quan tới tiền lương đóng BHXH, báo cáo của BHXH Việt Nam cho thấy: Nhiều doanh nghiệp vẫn chủ yếu xây dựng thang,bảng lương để đóng BHXH bằng mức thấp nhất. Tức là bằng mức lương tối thiểu vùng, cộng thêm 7 % đối với lao động đã qua đào tạo nghề và cộng thêm 5 % hoặc 7 % đối với lao động làm nghề hoặc công việc nặng nhọc, độc hại, nguy hiểm, đặc biệt nặng nhọc độc hại và nguy hiểm. Đối với lao động giữ chức vụ, khoảng 80 % doanh nghiệp đã xây dựng thang, bảng lương cụ thể theo chức danh. Đơn cử như với chức vụ giám đốc sản xuất, giám đốc điều hành, trưởng phòng. Còn lại các doanh nghiệp xây dựng đối với lao động giữ chức vụ theo thang lương, bảng lương chuyên môn nghiệp vụ và bảng phụ cấp chức vụ, phụ cấp trách nhiệm.Thống kê của BHXH Việt Nam cũng cho thấy, đa số doanh nghiệp đã đăng ký đóng BHXH cho người lao động theo mức lương mà không có khoản bổ sung khác. Mặc dù quy định từ ngày 1/1/2018, tiền lương tháng đóng BHXH gồm mức lương và thêm khoản bổ sung khác.\"\n",
        "# Tokenize văn bản đầu vào và chuyển sang tensor PyTorch trên GPU\n",
        "tokenized_text = tokenizer.encode(src, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Đưa mô hình vào chế độ đánh giá (tắt dropout, batchnorm, v.v.)\n",
        "model.eval()\n",
        "\n",
        "# Sinh tóm tắt bằng mô hình đã huấn luyện\n",
        "summary_ids = model.generate(\n",
        "                    tokenized_text,  # Đầu vào đã được mã hóa\n",
        "                    max_length=512,  # Độ dài tối đa của đoạn tóm tắt\n",
        "                    num_beams=5,  # Beam search với 5 beams để cải thiện chất lượng đầu ra\n",
        "                    repetition_penalty=2.5,  # Phạt lặp từ, giúp tránh việc lặp từ không cần thiết\n",
        "                    length_penalty=1.0,  # Hệ số phạt độ dài, giá trị >1 sẽ ưu tiên câu dài hơn\n",
        "                    early_stopping=True  # Dừng sớm nếu tất cả các beam đều tạo ra câu kết thúc\n",
        "                )\n",
        "\n",
        "# Giải mã đầu ra của mô hình từ token thành văn bản gốc, bỏ các token đặc biệt\n",
        "output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otwiOaiZc7oA"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_checkpoint = \"NlpHUST/t5-small-vi-summarization\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5FRv45PgcK-"
      },
      "source": [
        "#### Tiền xử lý dữ liệu, chia tập train - test - validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "napHs_tXeIjR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Đọc dữ liệu từ excel\n",
        "df = pd.read_excel('Research_T5\\\\Dataset\\\\summary_paper.xlsx', usecols=['body', 'abstract'])\n",
        "\n",
        "# Load tokenizer\n",
        "model_name = \"NlpHUST/t5-small-vi-summarization\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Hàm tiền xử lý dữ liệu\n",
        "class SummarizationDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_input_length=512, max_target_length=128):\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_input_length = max_input_length\n",
        "        self.max_target_length = max_target_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        content = self.data.iloc[idx][\"body\"]\n",
        "        summary = self.data.iloc[idx][\"abstract\"]\n",
        "\n",
        "        # Tokenize dữ liệu đầu vào\n",
        "        inputs = tokenizer(\"Tóm tắt: \" + content,\n",
        "                           max_length=self.max_input_length,\n",
        "                           truncation=True,\n",
        "                           return_tensors=\"pt\")\n",
        "\n",
        "        # Tokenize nhãn (tóm tắt)\n",
        "        labels = tokenizer(summary,\n",
        "                           max_length=self.max_target_length,\n",
        "                           truncation=True,\n",
        "                           return_tensors=\"pt\")\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": inputs[\"input_ids\"].squeeze(),\n",
        "            \"attention_mask\": inputs[\"attention_mask\"].squeeze(),\n",
        "            \"labels\": labels[\"input_ids\"].squeeze(),\n",
        "        }\n",
        "\n",
        "# Collate_fn tùy chỉnh để padding trong batch\n",
        "def collate_fn(batch, max_input_length=512, max_target_length=128):\n",
        "    input_ids = [item[\"input_ids\"] for item in batch]\n",
        "    attention_mask = [item[\"attention_mask\"] for item in batch]\n",
        "    labels = [item[\"labels\"] for item in batch]\n",
        "\n",
        "    # Padding động\n",
        "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
        "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels\n",
        "    }\n",
        "\n",
        "\n",
        "# Chia dữ liệu thành tập huấn luyện, kiểm định và kiểm thử (60%-20%-20%)\n",
        "train_size = int(0.6 * len(df))\n",
        "val_size = int(0.2 * len(df))\n",
        "\n",
        "train_df, temp_df = df[:train_size], df[train_size:]\n",
        "val_df, test_df = temp_df[:val_size], temp_df[val_size:]\n",
        "\n",
        "print(f\"Training data: {len(train_df)} examples\")\n",
        "print(f\"Validation data: {len(val_df)} examples\")\n",
        "print(f\"Test data: {len(test_df)} examples\")\n",
        "\n",
        "# DataLoader\n",
        "batch_size = 4\n",
        "accumulation_steps = 8  # Effective batch_size = 32 (4*8)\n",
        "\n",
        "train_dataset = SummarizationDataset(train_df, tokenizer, max_input_length=512)\n",
        "val_dataset = SummarizationDataset(val_df, tokenizer, max_input_length=512)\n",
        "test_dataset = SummarizationDataset(test_df, tokenizer, max_input_length=512)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                         collate_fn=lambda batch: collate_fn(batch, max_input_length=512, max_target_length=128))\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                       collate_fn=lambda batch: collate_fn(batch, max_input_length=512, max_target_length=128))\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                        collate_fn=lambda batch: collate_fn(batch, max_input_length=512, max_target_length=128))\n",
        "\n",
        "print(df.head())  # In 5 dòng đầu của DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "an8tjM18lam7"
      },
      "outputs": [],
      "source": [
        "content = df['body'][0]\n",
        "tokens = tokenizer(content, return_tensors=\"pt\")[\"input_ids\"]\n",
        "print(tokens.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqQLRRZ4YAnd"
      },
      "outputs": [],
      "source": [
        "# Calculate the length of each 'body' and find the maximum\n",
        "max_length = 0\n",
        "if 'body' in df.columns:\n",
        "  max_length = df['body'].str.len().max()\n",
        "else:\n",
        "  print(\"The column 'body' does not exist in the DataFrame.\")\n",
        "\n",
        "\n",
        "print(f\"The length of the longest sentence in the dataset is: {max_length}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV4v134dlaFA"
      },
      "source": [
        "#### Bắt đầu fine-tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-oul-64efBP"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
        "\n",
        "if checkpoint_files:\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJ1srcgMniC9"
      },
      "outputs": [],
      "source": [
        "%pip install bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in c:\\users\\84978\\anaconda3\\lib\\site-packages (4.66.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\84978\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eVnO0HPMGg8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from transformers import T5Tokenizer\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from bert_score import score as bert_score\n",
        "import nltk\n",
        "\n",
        "# Download NLTK resources if needed\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "# Hàm đánh giá\n",
        "def evaluate_metrics(model, val_loader, tokenizer, device):\n",
        "    model.eval()\n",
        "    predictions, references = [], []\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Evaluating\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"]\n",
        "            outputs = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_length=128,\n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5,\n",
        "                length_penalty=1.0,\n",
        "                early_stopping=True\n",
        "            )\n",
        "            preds = [tokenizer.decode(out, skip_special_tokens=True) for out in outputs]\n",
        "            refs = [tokenizer.decode(lab, skip_special_tokens=True) for lab in labels]\n",
        "            predictions.extend(preds)\n",
        "            references.extend(refs)\n",
        "\n",
        "    # ROUGE-1, 2, 3, L\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rouge3', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rouge3\": [], \"rougeL\": []}\n",
        "    for ref, pred in zip(references, predictions):\n",
        "        scores = scorer.score(ref, pred)\n",
        "        for key in rouge_scores:\n",
        "            rouge_scores[key].append(scores[key].fmeasure)\n",
        "\n",
        "    # BLEU-1, 2, 3, 4\n",
        "    smoothie = SmoothingFunction().method1\n",
        "    bleu_scores = {\"bleu1\": [], \"bleu2\": [], \"bleu3\": [], \"bleu4\": []}\n",
        "    for ref, pred in zip(references, predictions):\n",
        "        ref_tokens = nltk.word_tokenize(ref)\n",
        "        pred_tokens = nltk.word_tokenize(pred)\n",
        "        bleu_scores[\"bleu1\"].append(sentence_bleu([ref_tokens], pred_tokens, weights=(1, 0, 0, 0), smoothing_function=smoothie))\n",
        "        bleu_scores[\"bleu2\"].append(sentence_bleu([ref_tokens], pred_tokens, weights=(0, 1, 0, 0), smoothing_function=smoothie))\n",
        "        bleu_scores[\"bleu3\"].append(sentence_bleu([ref_tokens], pred_tokens, weights=(0, 0, 1, 0), smoothing_function=smoothie))\n",
        "        bleu_scores[\"bleu4\"].append(sentence_bleu([ref_tokens], pred_tokens, weights=(0, 0, 0, 1), smoothing_function=smoothie))\n",
        "\n",
        "    # BERTScore\n",
        "    P, R, F1 = bert_score(predictions, references, lang=\"vi\", verbose=True)\n",
        "    bertscore_f1 = F1.mean().item()\n",
        "\n",
        "    # Tổng hợp kết quả\n",
        "    results = {\n",
        "        \"ROUGE-1\": sum(rouge_scores[\"rouge1\"]) / len(rouge_scores[\"rouge1\"]),\n",
        "        \"ROUGE-2\": sum(rouge_scores[\"rouge2\"]) / len(rouge_scores[\"rouge2\"]),\n",
        "        \"ROUGE-3\": sum(rouge_scores[\"rouge3\"]) / len(rouge_scores[\"rouge3\"]),\n",
        "        \"ROUGE-L\": sum(rouge_scores[\"rougeL\"]) / len(rouge_scores[\"rougeL\"]),\n",
        "        \"BLEU-1\": sum(bleu_scores[\"bleu1\"]) / len(bleu_scores[\"bleu1\"]),\n",
        "        \"BLEU-2\": sum(bleu_scores[\"bleu2\"]) / len(bleu_scores[\"bleu2\"]),\n",
        "        \"BLEU-3\": sum(bleu_scores[\"bleu3\"]) / len(bleu_scores[\"bleu3\"]),\n",
        "        \"BLEU-4\": sum(bleu_scores[\"bleu4\"]) / len(bleu_scores[\"bleu4\"]),\n",
        "        \"BERTScore\": bertscore_f1\n",
        "    }\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrb60RudMe0i"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqNebUw_NgDi"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(f\"Bộ nhớ đã cấp: {torch.cuda.memory_allocated(device) / 1024**3:.2f} GiB\")\n",
        "    print(f\"Bộ nhớ giữ: {torch.cuda.memory_reserved(device) / 1024**3:.2f} GiB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDImpUIIeqEV"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Hàm lưu checkpoint\n",
        "def save_checkpoint(model, optimizer, epoch, checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)  # Tạo thư mục nếu chưa có\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch+1}.pt\")\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }, checkpoint_path)\n",
        "\n",
        "    print(f\"Đã lưu checkpoint tại {checkpoint_path}\")\n",
        "\n",
        "# Hàm huấn luyện có checkpoint\n",
        "def train_model(model, pretrained_model, train_loader, val_loader, test_loader, optimizer, device, epochs=10, checkpoint_dir=checkpoint_dir, start_epoch=0):\n",
        "    best_val_loss = float(\"inf\")\n",
        "    patience = 5\n",
        "    patience_counter = 0\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    '''\n",
        "    # Xóa bộ nhớ GPU trước khi bắt đầu huấn luyện\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"Deleted GPU memory before training\")\n",
        "    '''\n",
        "\n",
        "    # Tính tổng số bước trong một epoch (train + val)\n",
        "    total_steps_per_epoch = len(train_loader) + len(val_loader)\n",
        "\n",
        "    # Đánh giá mô hình pre-trained trên tập test trước khi fine-tune\n",
        "    print(\"Evaluating Pre-trained Model on Test set...\")\n",
        "    pretrained_metrics = evaluate_metrics(pretrained_model, test_loader, tokenizer, device)\n",
        "    print(\"Pre-trained Metrics:\")\n",
        "    for metric, value in pretrained_metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    '''\n",
        "    # Xóa bộ nhớ GPU sau khi đánh giá mô hình pre-trained\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        print(\"Deleted GPU memory after evaluating the pre-trained model\")\n",
        "    '''\n",
        "    # Bắt đầu fine-tune\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Một thanh tqdm duy nhất cho cả train và val\n",
        "        with tqdm(total=100, desc=f\"Epoch {epoch+1}/{epochs}\", unit=\"%\", bar_format=\"{l_bar}{bar}| {n:.0f}/{total:.0f}% [{elapsed}<{remaining}]\") as pbar:\n",
        "            # Train phase\n",
        "            steps_completed = 0\n",
        "            for i, batch in enumerate(train_loader):\n",
        "                input_ids = batch[\"input_ids\"].to(device)\n",
        "                attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                labels = batch[\"labels\"].to(device)\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss / accumulation_steps  # Chia nhỏ loss theo accumulation_steps\n",
        "                loss.backward()\n",
        "                if (i + 1) % accumulation_steps == 0:  # Cập nhật trọng số sau mỗi accumulation_steps\n",
        "                    optimizer.step()\n",
        "                    optimizer.zero_grad()\n",
        "                train_loss += loss.item() * accumulation_steps\n",
        "\n",
        "                # Cập nhật tiến trình\n",
        "                steps_completed += 1\n",
        "                progress = (steps_completed / total_steps_per_epoch) * 100\n",
        "                pbar.n = progress\n",
        "                pbar.refresh()\n",
        "\n",
        "            avg_train_loss = train_loss / len(train_loader)\n",
        "            train_losses.append(avg_train_loss)\n",
        "\n",
        "            # Val phase\n",
        "            model.eval()\n",
        "            val_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for batch in val_loader:\n",
        "                    input_ids = batch[\"input_ids\"].to(device)\n",
        "                    attention_mask = batch[\"attention_mask\"].to(device)\n",
        "                    labels = batch[\"labels\"].to(device)\n",
        "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                    val_loss += outputs.loss.item()\n",
        "\n",
        "                    # Cập nhật tiến trình\n",
        "                    steps_completed += 1\n",
        "                    progress = (steps_completed / total_steps_per_epoch) * 100\n",
        "                    pbar.n = progress\n",
        "                    pbar.refresh()\n",
        "\n",
        "            avg_val_loss = val_loss / len(val_loader)\n",
        "            val_losses.append(avg_val_loss)\n",
        "\n",
        "            # Đánh giá metrics trên val set\n",
        "            metrics = evaluate_metrics(model, val_loader, tokenizer, device)\n",
        "            print(f\"\\nEpoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}, Metrics: {metrics}\")\n",
        "\n",
        "        # Lưu checkpoint và xóa cache\n",
        "        save_checkpoint(model, optimizer, epoch, checkpoint_dir)\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            print(f\"Đã xóa cache GPU sau epoch {epoch+1}\")\n",
        "\n",
        "        # Early stopping và lưu mô hình tốt nhất\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            torch.save(model.state_dict(), os.path.join(checkpoint_dir, \"best_model.pt\"))\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "    # Đánh giá mô hình fine-tuned trên tập test\n",
        "    print(\"Evaluating Fine-tuned Model on Test set...\")\n",
        "    finetuned_metrics = evaluate_metrics(model, test_loader, tokenizer, device)\n",
        "    print(\"Fine-tuned Metrics:\")\n",
        "    for metric, value in finetuned_metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    # Vẽ biểu đồ Train loss và Val loss\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Train Loss\", marker='o')\n",
        "    plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Val Loss\", marker='o')\n",
        "    plt.title(\"Train Loss và Val Loss qua các Epoch\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nSo sánh Pre-trained vs Fine-tuned:\")\n",
        "    print(\"{:<15} {:<15} {:<15}\".format(\"Chỉ số\", \"Pre-trained\", \"Fine-tuned\"))\n",
        "    for metric in pretrained_metrics.keys():\n",
        "        print(\"{:<15} {:<15.4f} {:<15.4f}\".format(metric, pretrained_metrics[metric], finetuned_metrics[metric]))\n",
        "\n",
        "    return pretrained_metrics, finetuned_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erKaeS1_my3h",
        "outputId": "61671375-19b4-4094-8e5e-6d134b668562"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10:  23%|██▎       | 23/100% [03:13<10:45]"
          ]
        }
      ],
      "source": [
        "# Chạy quá trình fine-tune\n",
        "pretrained_metrics, finetuned_metrics = train_model(model, pretrained_model, train_loader, val_loader, test_loader, optimizer, device, epochs=10, checkpoint_dir=checkpoint_dir, start_epoch=start_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueyqt0z3et3N"
      },
      "outputs": [],
      "source": [
        "def summarize(text, model, tokenizer, device):\n",
        "    model.eval()\n",
        "    inputs = tokenizer(\"Tóm tắt: \" + text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
        "    with torch.no_grad():\n",
        "        summary_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=128, num_beams=4, early_stopping=True)\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "\n",
        "# Test với một câu\n",
        "test_text = df.iloc[0][\"body\"]\n",
        "print(\"Tóm tắt:\", summarize(test_text, model, tokenizer, device))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "t5_research",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
